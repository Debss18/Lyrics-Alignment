{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ddeef5b94d41425abcccffb95d621369",
        "deepnote_cell_type": "text-cell-h1",
        "id": "wRzVI4R7y_JD"
      },
      "source": [
        "# SongSay Lyrics Alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "e1d9bf1274934d48a0b9bc4cf306564f",
        "deepnote_cell_type": "text-cell-p",
        "id": "ptTZTh8-y_JH"
      },
      "source": [
        "Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "28ba4a2845b347dcbe1d21463801dcb6",
        "deepnote_cell_type": "text-cell-p",
        "id": "VG_fRgFqy_JH"
      },
      "source": [
        ".ass files - ground truth start time of each sentence; .txt files - previous predicted start time of each sentence; _all.txt files - previous predicted start time of each word in sentences; .wav files - 700 full sound tracks, roughly 3~5 minutes each; _source_3.wav files - separated vocal tracks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "62a5461035cd4a569516142e02766993",
        "deepnote_cell_type": "text-cell-p",
        "id": "gzfB0tjvy_JH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deepnote_to_be_reexecuted": true,
        "cell_id": "a62a7194f11840ae84de336263ffa5d1",
        "deepnote_cell_type": "code",
        "id": "PE1ytP0Yy_JJ"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##imports\n",
        "import madmom\n",
        "import crepe\n",
        "import boto3\n",
        "import os\n",
        "import scipy\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import librosa"
      ],
      "metadata": {
        "id": "bEdE9ADi2zfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading Audio Files"
      ],
      "metadata": {
        "id": "2c2Azf27JyDY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "5ef7b279",
        "execution_start": 1732661973451,
        "execution_millis": 338187,
        "execution_context_id": "2f92b2e2-b8a3-41c1-b523-8b2b9f6c8bf2",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "27eef000dda3450e8412ce777a1688af",
        "deepnote_cell_type": "code",
        "id": "ZP5clXYfy_JK",
        "outputId": "cedde0b8-13a9-4909-ed3d-0e2a8aa5fea8"
      },
      "source": [
        "# Ensure the download directory exists\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "# Initialize S3 client with credentials\n",
        "s3_client = boto3.client(\n",
        "    \"s3\",\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "    region_name=AWS_REGION,\n",
        ")\n",
        "\n",
        "def download_wav_files(bucket, prefix, download_dir, max_files=100):\n",
        "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
        "    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
        "\n",
        "    file_count = 0  # Counter for downloaded files\n",
        "\n",
        "    for page in pages:\n",
        "        if \"Contents\" in page:\n",
        "            for obj in page[\"Contents\"]:\n",
        "                if file_count >= max_files:\n",
        "                    print(f\"Reached the limit of {max_files} files. Stopping.\")\n",
        "                    return\n",
        "\n",
        "                key = obj[\"Key\"]\n",
        "                if key.endswith(\"source_3.wav\"):\n",
        "                    file_name = os.path.basename(key)\n",
        "                    local_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "                    print(f\"Downloading {key} to {local_path}...\")\n",
        "                    s3_client.download_file(bucket, key, local_path)\n",
        "                    print(f\"Downloaded {key}\")\n",
        "                    file_count += 1  # Increment the counter\n",
        "\n",
        "# Download the first 100 matching files\n",
        "download_wav_files(BUCKET_NAME, FOLDER_PREFIX, DOWNLOAD_DIR, max_files=100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading large-folder/1047_v360P_V1_source_3.wav to /work/audio/1047_v360P_V1_source_3.wav...\nDownloaded large-folder/1047_v360P_V1_source_3.wav\nDownloading large-folder/10617_v360P_V1_source_3.wav to /work/audio/10617_v360P_V1_source_3.wav...\nDownloaded large-folder/10617_v360P_V1_source_3.wav\nDownloading large-folder/10788_v360P_V1_source_3.wav to /work/audio/10788_v360P_V1_source_3.wav...\nDownloaded large-folder/10788_v360P_V1_source_3.wav\nDownloading large-folder/10932_v360P_V1_source_3.wav to /work/audio/10932_v360P_V1_source_3.wav...\nDownloaded large-folder/10932_v360P_V1_source_3.wav\nDownloading large-folder/10962_v360P_V1_source_3.wav to /work/audio/10962_v360P_V1_source_3.wav...\nDownloaded large-folder/10962_v360P_V1_source_3.wav\nDownloading large-folder/10965_v360P_V1_source_3.wav to /work/audio/10965_v360P_V1_source_3.wav...\nDownloaded large-folder/10965_v360P_V1_source_3.wav\nDownloading large-folder/10966_v360P_V1_source_3.wav to /work/audio/10966_v360P_V1_source_3.wav...\nDownloaded large-folder/10966_v360P_V1_source_3.wav\nDownloading large-folder/11018_v360P_V1_source_3.wav to /work/audio/11018_v360P_V1_source_3.wav...\nDownloaded large-folder/11018_v360P_V1_source_3.wav\nDownloading large-folder/11027_v360P_V1_source_3.wav to /work/audio/11027_v360P_V1_source_3.wav...\nDownloaded large-folder/11027_v360P_V1_source_3.wav\nDownloading large-folder/11064_v360P_V1_source_3.wav to /work/audio/11064_v360P_V1_source_3.wav...\nDownloaded large-folder/11064_v360P_V1_source_3.wav\nDownloading large-folder/11067_v360P_V1_source_3.wav to /work/audio/11067_v360P_V1_source_3.wav...\nDownloaded large-folder/11067_v360P_V1_source_3.wav\nDownloading large-folder/11094_v360P_V1_source_3.wav to /work/audio/11094_v360P_V1_source_3.wav...\nDownloaded large-folder/11094_v360P_V1_source_3.wav\nDownloading large-folder/11099_v360P_V1_source_3.wav to /work/audio/11099_v360P_V1_source_3.wav...\nDownloaded large-folder/11099_v360P_V1_source_3.wav\nDownloading large-folder/11108_v360P_V1_source_3.wav to /work/audio/11108_v360P_V1_source_3.wav...\nDownloaded large-folder/11108_v360P_V1_source_3.wav\nDownloading large-folder/11141_v360P_V1_source_3.wav to /work/audio/11141_v360P_V1_source_3.wav...\nDownloaded large-folder/11141_v360P_V1_source_3.wav\nDownloading large-folder/11173_v360P_V1_source_3.wav to /work/audio/11173_v360P_V1_source_3.wav...\nDownloaded large-folder/11173_v360P_V1_source_3.wav\nDownloading large-folder/11196_v360P_V1_source_3.wav to /work/audio/11196_v360P_V1_source_3.wav...\nDownloaded large-folder/11196_v360P_V1_source_3.wav\nDownloading large-folder/11198_v360P_V1_source_3.wav to /work/audio/11198_v360P_V1_source_3.wav...\nDownloaded large-folder/11198_v360P_V1_source_3.wav\nDownloading large-folder/11205_v360P_V1_source_3.wav to /work/audio/11205_v360P_V1_source_3.wav...\nDownloaded large-folder/11205_v360P_V1_source_3.wav\nDownloading large-folder/11405_v360P_V1_source_3.wav to /work/audio/11405_v360P_V1_source_3.wav...\nDownloaded large-folder/11405_v360P_V1_source_3.wav\nDownloading large-folder/11410_v360P_V1_source_3.wav to /work/audio/11410_v360P_V1_source_3.wav...\nDownloaded large-folder/11410_v360P_V1_source_3.wav\nDownloading large-folder/11432_v360P_V1_source_3.wav to /work/audio/11432_v360P_V1_source_3.wav...\nDownloaded large-folder/11432_v360P_V1_source_3.wav\nDownloading large-folder/11580_v360P_V1_source_3.wav to /work/audio/11580_v360P_V1_source_3.wav...\nDownloaded large-folder/11580_v360P_V1_source_3.wav\nDownloading large-folder/11595_v360P_V1_source_3.wav to /work/audio/11595_v360P_V1_source_3.wav...\nDownloaded large-folder/11595_v360P_V1_source_3.wav\nDownloading large-folder/11626_v360P_V1_source_3.wav to /work/audio/11626_v360P_V1_source_3.wav...\nDownloaded large-folder/11626_v360P_V1_source_3.wav\nDownloading large-folder/11641_v360P_V1_source_3.wav to /work/audio/11641_v360P_V1_source_3.wav...\nDownloaded large-folder/11641_v360P_V1_source_3.wav\nDownloading large-folder/11658_v360P_V1_source_3.wav to /work/audio/11658_v360P_V1_source_3.wav...\nDownloaded large-folder/11658_v360P_V1_source_3.wav\nDownloading large-folder/11815_v360P_V1_source_3.wav to /work/audio/11815_v360P_V1_source_3.wav...\nDownloaded large-folder/11815_v360P_V1_source_3.wav\nDownloading large-folder/11818_v360P_V1_source_3.wav to /work/audio/11818_v360P_V1_source_3.wav...\nDownloaded large-folder/11818_v360P_V1_source_3.wav\nDownloading large-folder/11890_v360P_V1_source_3.wav to /work/audio/11890_v360P_V1_source_3.wav...\nDownloaded large-folder/11890_v360P_V1_source_3.wav\nDownloading large-folder/11892_v360P_V1_source_3.wav to /work/audio/11892_v360P_V1_source_3.wav...\nDownloaded large-folder/11892_v360P_V1_source_3.wav\nDownloading large-folder/11904_v360P_V1_source_3.wav to /work/audio/11904_v360P_V1_source_3.wav...\nDownloaded large-folder/11904_v360P_V1_source_3.wav\nDownloading large-folder/12013_v360P_V1_source_3.wav to /work/audio/12013_v360P_V1_source_3.wav...\nDownloaded large-folder/12013_v360P_V1_source_3.wav\nDownloading large-folder/12028_v360P_V1_source_3.wav to /work/audio/12028_v360P_V1_source_3.wav...\nDownloaded large-folder/12028_v360P_V1_source_3.wav\nDownloading large-folder/12030_v360P_V1_source_3.wav to /work/audio/12030_v360P_V1_source_3.wav...\nDownloaded large-folder/12030_v360P_V1_source_3.wav\nDownloading large-folder/12037_v360P_V1_source_3.wav to /work/audio/12037_v360P_V1_source_3.wav...\nDownloaded large-folder/12037_v360P_V1_source_3.wav\nDownloading large-folder/12121_v360P_V1_source_3.wav to /work/audio/12121_v360P_V1_source_3.wav...\nDownloaded large-folder/12121_v360P_V1_source_3.wav\nDownloading large-folder/12164_v360P_V1_source_3.wav to /work/audio/12164_v360P_V1_source_3.wav...\nDownloaded large-folder/12164_v360P_V1_source_3.wav\nDownloading large-folder/12205_v360P_V1_source_3.wav to /work/audio/12205_v360P_V1_source_3.wav...\nDownloaded large-folder/12205_v360P_V1_source_3.wav\nDownloading large-folder/12254_v360P_V1_source_3.wav to /work/audio/12254_v360P_V1_source_3.wav...\nDownloaded large-folder/12254_v360P_V1_source_3.wav\nDownloading large-folder/12295_v360P_V1_source_3.wav to /work/audio/12295_v360P_V1_source_3.wav...\nDownloaded large-folder/12295_v360P_V1_source_3.wav\nDownloading large-folder/12305_v360P_V1_source_3.wav to /work/audio/12305_v360P_V1_source_3.wav...\nDownloaded large-folder/12305_v360P_V1_source_3.wav\nDownloading large-folder/12367_v360P_V1_source_3.wav to /work/audio/12367_v360P_V1_source_3.wav...\nDownloaded large-folder/12367_v360P_V1_source_3.wav\nDownloading large-folder/12412_v360P_V1_source_3.wav to /work/audio/12412_v360P_V1_source_3.wav...\nDownloaded large-folder/12412_v360P_V1_source_3.wav\nDownloading large-folder/12429_v360P_V1_source_3.wav to /work/audio/12429_v360P_V1_source_3.wav...\nDownloaded large-folder/12429_v360P_V1_source_3.wav\nDownloading large-folder/12478_v360P_V1_source_3.wav to /work/audio/12478_v360P_V1_source_3.wav...\nDownloaded large-folder/12478_v360P_V1_source_3.wav\nDownloading large-folder/12480_v360P_V1_source_3.wav to /work/audio/12480_v360P_V1_source_3.wav...\nDownloaded large-folder/12480_v360P_V1_source_3.wav\nDownloading large-folder/12562_v360P_V1_source_3.wav to /work/audio/12562_v360P_V1_source_3.wav...\nDownloaded large-folder/12562_v360P_V1_source_3.wav\nDownloading large-folder/12570_v360P_V1_source_3.wav to /work/audio/12570_v360P_V1_source_3.wav...\nDownloaded large-folder/12570_v360P_V1_source_3.wav\nDownloading large-folder/12572_v360P_V1_source_3.wav to /work/audio/12572_v360P_V1_source_3.wav...\nDownloaded large-folder/12572_v360P_V1_source_3.wav\nDownloading large-folder/12612_v360P_V1_source_3.wav to /work/audio/12612_v360P_V1_source_3.wav...\nDownloaded large-folder/12612_v360P_V1_source_3.wav\nDownloading large-folder/12616_v360P_V1_source_3.wav to /work/audio/12616_v360P_V1_source_3.wav...\nDownloaded large-folder/12616_v360P_V1_source_3.wav\nDownloading large-folder/12619_v360P_V1_source_3.wav to /work/audio/12619_v360P_V1_source_3.wav...\nDownloaded large-folder/12619_v360P_V1_source_3.wav\nDownloading large-folder/12633_v360P_V1_source_3.wav to /work/audio/12633_v360P_V1_source_3.wav...\nDownloaded large-folder/12633_v360P_V1_source_3.wav\nDownloading large-folder/12643_v360P_V1_source_3.wav to /work/audio/12643_v360P_V1_source_3.wav...\nDownloaded large-folder/12643_v360P_V1_source_3.wav\nDownloading large-folder/1264_v360P_V1_source_3.wav to /work/audio/1264_v360P_V1_source_3.wav...\nDownloaded large-folder/1264_v360P_V1_source_3.wav\nDownloading large-folder/12679_v360P_V1_source_3.wav to /work/audio/12679_v360P_V1_source_3.wav...\nDownloaded large-folder/12679_v360P_V1_source_3.wav\nDownloading large-folder/12686_v360P_V1_source_3.wav to /work/audio/12686_v360P_V1_source_3.wav...\nDownloaded large-folder/12686_v360P_V1_source_3.wav\nDownloading large-folder/12706_v360P_V1_source_3.wav to /work/audio/12706_v360P_V1_source_3.wav...\nDownloaded large-folder/12706_v360P_V1_source_3.wav\nDownloading large-folder/12710_v360P_V1_source_3.wav to /work/audio/12710_v360P_V1_source_3.wav...\nDownloaded large-folder/12710_v360P_V1_source_3.wav\nDownloading large-folder/12711_v360P_V1_source_3.wav to /work/audio/12711_v360P_V1_source_3.wav...\nDownloaded large-folder/12711_v360P_V1_source_3.wav\nDownloading large-folder/12756_v360P_V1_source_3.wav to /work/audio/12756_v360P_V1_source_3.wav...\nDownloaded large-folder/12756_v360P_V1_source_3.wav\nDownloading large-folder/12757_v360P_V1_source_3.wav to /work/audio/12757_v360P_V1_source_3.wav...\nDownloaded large-folder/12757_v360P_V1_source_3.wav\nDownloading large-folder/12776_v360P_V1_source_3.wav to /work/audio/12776_v360P_V1_source_3.wav...\nDownloaded large-folder/12776_v360P_V1_source_3.wav\nDownloading large-folder/12789_v360P_V1_source_3.wav to /work/audio/12789_v360P_V1_source_3.wav...\nDownloaded large-folder/12789_v360P_V1_source_3.wav\nDownloading large-folder/12873_v360P_V1_source_3.wav to /work/audio/12873_v360P_V1_source_3.wav...\nDownloaded large-folder/12873_v360P_V1_source_3.wav\nDownloading large-folder/12877_v360P_V1_source_3.wav to /work/audio/12877_v360P_V1_source_3.wav...\nDownloaded large-folder/12877_v360P_V1_source_3.wav\nDownloading large-folder/12931_v360P_V1_source_3.wav to /work/audio/12931_v360P_V1_source_3.wav...\nDownloaded large-folder/12931_v360P_V1_source_3.wav\nDownloading large-folder/12964_v360P_V1_source_3.wav to /work/audio/12964_v360P_V1_source_3.wav...\nDownloaded large-folder/12964_v360P_V1_source_3.wav\nDownloading large-folder/13016_v360P_V1_source_3.wav to /work/audio/13016_v360P_V1_source_3.wav...\nDownloaded large-folder/13016_v360P_V1_source_3.wav\nDownloading large-folder/13017_v360P_V1_source_3.wav to /work/audio/13017_v360P_V1_source_3.wav...\nDownloaded large-folder/13017_v360P_V1_source_3.wav\nDownloading large-folder/13018_v360P_V1_source_3.wav to /work/audio/13018_v360P_V1_source_3.wav...\nDownloaded large-folder/13018_v360P_V1_source_3.wav\nDownloading large-folder/13069_v360P_V1_source_3.wav to /work/audio/13069_v360P_V1_source_3.wav...\nDownloaded large-folder/13069_v360P_V1_source_3.wav\nDownloading large-folder/13103_v360P_V1_source_3.wav to /work/audio/13103_v360P_V1_source_3.wav...\nDownloaded large-folder/13103_v360P_V1_source_3.wav\nDownloading large-folder/13117_v360P_V1_source_3.wav to /work/audio/13117_v360P_V1_source_3.wav...\nDownloaded large-folder/13117_v360P_V1_source_3.wav\nDownloading large-folder/13226_v360P_V1_source_3.wav to /work/audio/13226_v360P_V1_source_3.wav...\nDownloaded large-folder/13226_v360P_V1_source_3.wav\nDownloading large-folder/13283_v360P_V1_source_3.wav to /work/audio/13283_v360P_V1_source_3.wav...\nDownloaded large-folder/13283_v360P_V1_source_3.wav\nDownloading large-folder/13397_v360P_V1_source_3.wav to /work/audio/13397_v360P_V1_source_3.wav...\nDownloaded large-folder/13397_v360P_V1_source_3.wav\nDownloading large-folder/13407_v360P_V1_source_3.wav to /work/audio/13407_v360P_V1_source_3.wav...\nDownloaded large-folder/13407_v360P_V1_source_3.wav\nDownloading large-folder/13497_v360P_V1_source_3.wav to /work/audio/13497_v360P_V1_source_3.wav...\nDownloaded large-folder/13497_v360P_V1_source_3.wav\nDownloading large-folder/13647_v360P_V1_source_3.wav to /work/audio/13647_v360P_V1_source_3.wav...\nDownloaded large-folder/13647_v360P_V1_source_3.wav\nDownloading large-folder/13741_v360P_V1_source_3.wav to /work/audio/13741_v360P_V1_source_3.wav...\nDownloaded large-folder/13741_v360P_V1_source_3.wav\nDownloading large-folder/13742_v360P_V1_source_3.wav to /work/audio/13742_v360P_V1_source_3.wav...\nDownloaded large-folder/13742_v360P_V1_source_3.wav\nDownloading large-folder/13859_v360P_V1_source_3.wav to /work/audio/13859_v360P_V1_source_3.wav...\nDownloaded large-folder/13859_v360P_V1_source_3.wav\nDownloading large-folder/13888_v360P_V1_source_3.wav to /work/audio/13888_v360P_V1_source_3.wav...\nDownloaded large-folder/13888_v360P_V1_source_3.wav\nDownloading large-folder/13914_v360P_V1_source_3.wav to /work/audio/13914_v360P_V1_source_3.wav...\nDownloaded large-folder/13914_v360P_V1_source_3.wav\nDownloading large-folder/13935_v360P_V1_source_3.wav to /work/audio/13935_v360P_V1_source_3.wav...\nDownloaded large-folder/13935_v360P_V1_source_3.wav\nDownloading large-folder/13967_v360P_V1_source_3.wav to /work/audio/13967_v360P_V1_source_3.wav...\nDownloaded large-folder/13967_v360P_V1_source_3.wav\nDownloading large-folder/13969_v360P_V1_source_3.wav to /work/audio/13969_v360P_V1_source_3.wav...\nDownloaded large-folder/13969_v360P_V1_source_3.wav\nDownloading large-folder/13972_v360P_V1_source_3.wav to /work/audio/13972_v360P_V1_source_3.wav...\nDownloaded large-folder/13972_v360P_V1_source_3.wav\nDownloading large-folder/14022_v360P_V1_source_3.wav to /work/audio/14022_v360P_V1_source_3.wav...\nDownloaded large-folder/14022_v360P_V1_source_3.wav\nDownloading large-folder/14081_v360P_V1_source_3.wav to /work/audio/14081_v360P_V1_source_3.wav...\nDownloaded large-folder/14081_v360P_V1_source_3.wav\nDownloading large-folder/14120_v360P_V1_source_3.wav to /work/audio/14120_v360P_V1_source_3.wav...\nDownloaded large-folder/14120_v360P_V1_source_3.wav\nDownloading large-folder/14122_v360P_V1_source_3.wav to /work/audio/14122_v360P_V1_source_3.wav...\nDownloaded large-folder/14122_v360P_V1_source_3.wav\nDownloading large-folder/14123_v360P_V1_source_3.wav to /work/audio/14123_v360P_V1_source_3.wav...\nDownloaded large-folder/14123_v360P_V1_source_3.wav\nDownloading large-folder/14130_v360P_V1_source_3.wav to /work/audio/14130_v360P_V1_source_3.wav...\nDownloaded large-folder/14130_v360P_V1_source_3.wav\nDownloading large-folder/14142_v360P_V1_source_3.wav to /work/audio/14142_v360P_V1_source_3.wav...\nDownloaded large-folder/14142_v360P_V1_source_3.wav\nDownloading large-folder/14170_v360P_V1_source_3.wav to /work/audio/14170_v360P_V1_source_3.wav...\nDownloaded large-folder/14170_v360P_V1_source_3.wav\nDownloading large-folder/14182_v360P_V1_source_3.wav to /work/audio/14182_v360P_V1_source_3.wav...\nDownloaded large-folder/14182_v360P_V1_source_3.wav\nDownloading large-folder/14183_v360P_V1_source_3.wav to /work/audio/14183_v360P_V1_source_3.wav...\nDownloaded large-folder/14183_v360P_V1_source_3.wav\nReached the limit of 100 files. Stopping.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading and Processing Text Files"
      ],
      "metadata": {
        "id": "VyteUGgeJ-7J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "90dd5af3",
        "execution_start": 1732685401584,
        "execution_millis": 1171,
        "execution_context_id": "23fe59fd-fdb6-476b-8e0e-561dc3f3ee7d",
        "cell_id": "f1ef7397578f4e6e8195a194458ed014",
        "deepnote_cell_type": "code",
        "id": "i_DaXdVzy_JI"
      },
      "source": [
        "# Ensure the download directory exists\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "# Define S3 bucket and folder\n",
        "BUCKET_NAME = \"sagemaker-eu-north-1-205098052109\"\n",
        "FOLDER_PREFIX = \"large-folder/\"\n",
        "\n",
        "# # AWS credentials\n",
        "AWS_ACCESS_KEY_ID = \"\"\n",
        "AWS_SECRET_ACCESS_KEY = \"\"\n",
        "AWS_REGION = \"\"\n",
        "\n",
        "# Initialize S3 client with credentials\n",
        "s3_client = boto3.client(\n",
        "    \"s3\",\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "    region_name=AWS_REGION,\n",
        ")\n",
        "\n",
        "def download_txt_files(bucket, prefix, download_dir):\n",
        "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
        "    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
        "\n",
        "    for page in tqdm(pages):\n",
        "        if \"Contents\" in page:\n",
        "            for obj in page[\"Contents\"]:\n",
        "                key = obj[\"Key\"]\n",
        "                if key.endswith(\".txt\"):\n",
        "                    file_name = os.path.basename(key)\n",
        "                    local_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "                    print(f\"Downloading {key} to {local_path}...\")\n",
        "                    s3_client.download_file(bucket, key, local_path)\n",
        "                    print(f\"Downloaded {key}\")\n",
        "\n",
        "# Download all .txt files\n",
        "download_txt_files(BUCKET_NAME, FOLDER_PREFIX, DOWNLOAD_DIR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "cdc09efb",
        "execution_start": 1733265509733,
        "execution_millis": 68237,
        "execution_context_id": "9ff8ea37-5664-4ca5-a559-0b2cdacda485",
        "cell_id": "8a54b7840e104698b95403df3de1c310",
        "deepnote_cell_type": "code",
        "id": "7bWNNXn_y_JK",
        "outputId": "888add63-0c27-4536-d655-5339db70e524"
      },
      "source": [
        "# Ensure the download directory exists\n",
        "DOWNLOAD_DIR = \"/work/ass_files/\"\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "# Initialize S3 client with credentials\n",
        "s3_client = boto3.client(\n",
        "    \"s3\",\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "    region_name=AWS_REGION,\n",
        ")\n",
        "\n",
        "def download_ass_files(bucket, prefix, download_dir, max_files=100):\n",
        "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
        "    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
        "\n",
        "    file_count = 0  # Counter for downloaded files\n",
        "\n",
        "    for page in pages:\n",
        "        if \"Contents\" in page:\n",
        "            for obj in page[\"Contents\"]:\n",
        "                if file_count >= max_files:\n",
        "                    print(f\"Reached the limit of {max_files} files. Stopping.\")\n",
        "                    return\n",
        "\n",
        "                key = obj[\"Key\"]\n",
        "                if key.endswith(\".ass\"):\n",
        "                    file_name = os.path.basename(key)\n",
        "                    local_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "                    print(f\"Downloading {key} to {local_path}...\")\n",
        "                    s3_client.download_file(bucket, key, local_path)\n",
        "                    print(f\"Downloaded {key}\")\n",
        "                    file_count += 1\n",
        "\n",
        "# Download the first 100 matching files\n",
        "download_ass_files(BUCKET_NAME, FOLDER_PREFIX, DOWNLOAD_DIR, max_files=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading large-folder/10284_V3.ass to /work/ass_files/10284_V3.ass...\nDownloaded large-folder/10284_V3.ass\nDownloading large-folder/10417_V3.ass to /work/ass_files/10417_V3.ass...\nDownloaded large-folder/10417_V3.ass\nDownloading large-folder/1047_V3.ass to /work/ass_files/1047_V3.ass...\nDownloaded large-folder/1047_V3.ass\nDownloading large-folder/10617_V3.ass to /work/ass_files/10617_V3.ass...\nDownloaded large-folder/10617_V3.ass\nDownloading large-folder/10662_V2.ass to /work/ass_files/10662_V2.ass...\nDownloaded large-folder/10662_V2.ass\nDownloading large-folder/10686_V2.ass to /work/ass_files/10686_V2.ass...\nDownloaded large-folder/10686_V2.ass\nDownloading large-folder/10739_V2.ass to /work/ass_files/10739_V2.ass...\nDownloaded large-folder/10739_V2.ass\nDownloading large-folder/10788_V3.ass to /work/ass_files/10788_V3.ass...\nDownloaded large-folder/10788_V3.ass\nDownloading large-folder/10932_V3.ass to /work/ass_files/10932_V3.ass...\nDownloaded large-folder/10932_V3.ass\nDownloading large-folder/10962_V3.ass to /work/ass_files/10962_V3.ass...\nDownloaded large-folder/10962_V3.ass\nDownloading large-folder/10965_V3.ass to /work/ass_files/10965_V3.ass...\nDownloaded large-folder/10965_V3.ass\nDownloading large-folder/10966_V3.ass to /work/ass_files/10966_V3.ass...\nDownloaded large-folder/10966_V3.ass\nDownloading large-folder/11018_V3.ass to /work/ass_files/11018_V3.ass...\nDownloaded large-folder/11018_V3.ass\nDownloading large-folder/11027_V3.ass to /work/ass_files/11027_V3.ass...\nDownloaded large-folder/11027_V3.ass\nDownloading large-folder/11064_V3.ass to /work/ass_files/11064_V3.ass...\nDownloaded large-folder/11064_V3.ass\nDownloading large-folder/11067_V3.ass to /work/ass_files/11067_V3.ass...\nDownloaded large-folder/11067_V3.ass\nDownloading large-folder/11094_V3.ass to /work/ass_files/11094_V3.ass...\nDownloaded large-folder/11094_V3.ass\nDownloading large-folder/11099_V3.ass to /work/ass_files/11099_V3.ass...\nDownloaded large-folder/11099_V3.ass\nDownloading large-folder/11108_V3.ass to /work/ass_files/11108_V3.ass...\nDownloaded large-folder/11108_V3.ass\nDownloading large-folder/11141_V3.ass to /work/ass_files/11141_V3.ass...\nDownloaded large-folder/11141_V3.ass\nDownloading large-folder/11173_V3.ass to /work/ass_files/11173_V3.ass...\nDownloaded large-folder/11173_V3.ass\nDownloading large-folder/11196_V3.ass to /work/ass_files/11196_V3.ass...\nDownloaded large-folder/11196_V3.ass\nDownloading large-folder/11198_V3.ass to /work/ass_files/11198_V3.ass...\nDownloaded large-folder/11198_V3.ass\nDownloading large-folder/11205_V3.ass to /work/ass_files/11205_V3.ass...\nDownloaded large-folder/11205_V3.ass\nDownloading large-folder/11405_V3.ass to /work/ass_files/11405_V3.ass...\nDownloaded large-folder/11405_V3.ass\nDownloading large-folder/11410_V3.ass to /work/ass_files/11410_V3.ass...\nDownloaded large-folder/11410_V3.ass\nDownloading large-folder/11432_V3.ass to /work/ass_files/11432_V3.ass...\nDownloaded large-folder/11432_V3.ass\nDownloading large-folder/11580_V3.ass to /work/ass_files/11580_V3.ass...\nDownloaded large-folder/11580_V3.ass\nDownloading large-folder/11595_V3.ass to /work/ass_files/11595_V3.ass...\nDownloaded large-folder/11595_V3.ass\nDownloading large-folder/11626_V3.ass to /work/ass_files/11626_V3.ass...\nDownloaded large-folder/11626_V3.ass\nDownloading large-folder/11641_V3.ass to /work/ass_files/11641_V3.ass...\nDownloaded large-folder/11641_V3.ass\nDownloading large-folder/11658_V3.ass to /work/ass_files/11658_V3.ass...\nDownloaded large-folder/11658_V3.ass\nDownloading large-folder/11815_V3.ass to /work/ass_files/11815_V3.ass...\nDownloaded large-folder/11815_V3.ass\nDownloading large-folder/11818_V3.ass to /work/ass_files/11818_V3.ass...\nDownloaded large-folder/11818_V3.ass\nDownloading large-folder/11890_V3.ass to /work/ass_files/11890_V3.ass...\nDownloaded large-folder/11890_V3.ass\nDownloading large-folder/11892_V3.ass to /work/ass_files/11892_V3.ass...\nDownloaded large-folder/11892_V3.ass\nDownloading large-folder/11904_V3.ass to /work/ass_files/11904_V3.ass...\nDownloaded large-folder/11904_V3.ass\nDownloading large-folder/12013_V3.ass to /work/ass_files/12013_V3.ass...\nDownloaded large-folder/12013_V3.ass\nDownloading large-folder/12028_V2.ass to /work/ass_files/12028_V2.ass...\nDownloaded large-folder/12028_V2.ass\nDownloading large-folder/12030_V3.ass to /work/ass_files/12030_V3.ass...\nDownloaded large-folder/12030_V3.ass\nDownloading large-folder/12037_V3.ass to /work/ass_files/12037_V3.ass...\nDownloaded large-folder/12037_V3.ass\nDownloading large-folder/12121_V3.ass to /work/ass_files/12121_V3.ass...\nDownloaded large-folder/12121_V3.ass\nDownloading large-folder/12164_V3.ass to /work/ass_files/12164_V3.ass...\nDownloaded large-folder/12164_V3.ass\nDownloading large-folder/12205_V3.ass to /work/ass_files/12205_V3.ass...\nDownloaded large-folder/12205_V3.ass\nDownloading large-folder/12254_V3.ass to /work/ass_files/12254_V3.ass...\nDownloaded large-folder/12254_V3.ass\nDownloading large-folder/12295_V3.ass to /work/ass_files/12295_V3.ass...\nDownloaded large-folder/12295_V3.ass\nDownloading large-folder/12305_V3.ass to /work/ass_files/12305_V3.ass...\nDownloaded large-folder/12305_V3.ass\nDownloading large-folder/12367_V3.ass to /work/ass_files/12367_V3.ass...\nDownloaded large-folder/12367_V3.ass\nDownloading large-folder/12412_V3.ass to /work/ass_files/12412_V3.ass...\nDownloaded large-folder/12412_V3.ass\nDownloading large-folder/12429_V3.ass to /work/ass_files/12429_V3.ass...\nDownloaded large-folder/12429_V3.ass\nDownloading large-folder/12478_V3.ass to /work/ass_files/12478_V3.ass...\nDownloaded large-folder/12478_V3.ass\nDownloading large-folder/12480_V3.ass to /work/ass_files/12480_V3.ass...\nDownloaded large-folder/12480_V3.ass\nDownloading large-folder/12562_V3.ass to /work/ass_files/12562_V3.ass...\nDownloaded large-folder/12562_V3.ass\nDownloading large-folder/12570_V3.ass to /work/ass_files/12570_V3.ass...\nDownloaded large-folder/12570_V3.ass\nDownloading large-folder/12572_V3.ass to /work/ass_files/12572_V3.ass...\nDownloaded large-folder/12572_V3.ass\nDownloading large-folder/12612_V3.ass to /work/ass_files/12612_V3.ass...\nDownloaded large-folder/12612_V3.ass\nDownloading large-folder/12616_V3.ass to /work/ass_files/12616_V3.ass...\nDownloaded large-folder/12616_V3.ass\nDownloading large-folder/12619_V3.ass to /work/ass_files/12619_V3.ass...\nDownloaded large-folder/12619_V3.ass\nDownloading large-folder/12633_V3.ass to /work/ass_files/12633_V3.ass...\nDownloaded large-folder/12633_V3.ass\nDownloading large-folder/12643_V3.ass to /work/ass_files/12643_V3.ass...\nDownloaded large-folder/12643_V3.ass\nDownloading large-folder/1264_V3.ass to /work/ass_files/1264_V3.ass...\nDownloaded large-folder/1264_V3.ass\nDownloading large-folder/12679_V3.ass to /work/ass_files/12679_V3.ass...\nDownloaded large-folder/12679_V3.ass\nDownloading large-folder/12686_V3.ass to /work/ass_files/12686_V3.ass...\nDownloaded large-folder/12686_V3.ass\nDownloading large-folder/12706_V3.ass to /work/ass_files/12706_V3.ass...\nDownloaded large-folder/12706_V3.ass\nDownloading large-folder/12710_V3.ass to /work/ass_files/12710_V3.ass...\nDownloaded large-folder/12710_V3.ass\nDownloading large-folder/12711_V3.ass to /work/ass_files/12711_V3.ass...\nDownloaded large-folder/12711_V3.ass\nDownloading large-folder/12756_V3.ass to /work/ass_files/12756_V3.ass...\nDownloaded large-folder/12756_V3.ass\nDownloading large-folder/12757_V3.ass to /work/ass_files/12757_V3.ass...\nDownloaded large-folder/12757_V3.ass\nDownloading large-folder/12776_V3.ass to /work/ass_files/12776_V3.ass...\nDownloaded large-folder/12776_V3.ass\nDownloading large-folder/12789_V3.ass to /work/ass_files/12789_V3.ass...\nDownloaded large-folder/12789_V3.ass\nDownloading large-folder/12873_V3.ass to /work/ass_files/12873_V3.ass...\nDownloaded large-folder/12873_V3.ass\nDownloading large-folder/12877_V3.ass to /work/ass_files/12877_V3.ass...\nDownloaded large-folder/12877_V3.ass\nDownloading large-folder/12931_V3.ass to /work/ass_files/12931_V3.ass...\nDownloaded large-folder/12931_V3.ass\nDownloading large-folder/12964_V3.ass to /work/ass_files/12964_V3.ass...\nDownloaded large-folder/12964_V3.ass\nDownloading large-folder/13016_V3.ass to /work/ass_files/13016_V3.ass...\nDownloaded large-folder/13016_V3.ass\nDownloading large-folder/13017_V3.ass to /work/ass_files/13017_V3.ass...\nDownloaded large-folder/13017_V3.ass\nDownloading large-folder/13018_V3.ass to /work/ass_files/13018_V3.ass...\nDownloaded large-folder/13018_V3.ass\nDownloading large-folder/13069_V2.ass to /work/ass_files/13069_V2.ass...\nDownloaded large-folder/13069_V2.ass\nDownloading large-folder/13103_V3.ass to /work/ass_files/13103_V3.ass...\nDownloaded large-folder/13103_V3.ass\nDownloading large-folder/13117_V3.ass to /work/ass_files/13117_V3.ass...\nDownloaded large-folder/13117_V3.ass\nDownloading large-folder/13226_V3.ass to /work/ass_files/13226_V3.ass...\nDownloaded large-folder/13226_V3.ass\nDownloading large-folder/13283_V3.ass to /work/ass_files/13283_V3.ass...\nDownloaded large-folder/13283_V3.ass\nDownloading large-folder/13397_V3.ass to /work/ass_files/13397_V3.ass...\nDownloaded large-folder/13397_V3.ass\nDownloading large-folder/13407_V3.ass to /work/ass_files/13407_V3.ass...\nDownloaded large-folder/13407_V3.ass\nDownloading large-folder/13497_V3.ass to /work/ass_files/13497_V3.ass...\nDownloaded large-folder/13497_V3.ass\nDownloading large-folder/13647_V3.ass to /work/ass_files/13647_V3.ass...\nDownloaded large-folder/13647_V3.ass\nDownloading large-folder/13741_V3.ass to /work/ass_files/13741_V3.ass...\nDownloaded large-folder/13741_V3.ass\nDownloading large-folder/13742_V3.ass to /work/ass_files/13742_V3.ass...\nDownloaded large-folder/13742_V3.ass\nDownloading large-folder/13859_V3.ass to /work/ass_files/13859_V3.ass...\nDownloaded large-folder/13859_V3.ass\nDownloading large-folder/13888_V3.ass to /work/ass_files/13888_V3.ass...\nDownloaded large-folder/13888_V3.ass\nDownloading large-folder/13914_V2.ass to /work/ass_files/13914_V2.ass...\nDownloaded large-folder/13914_V2.ass\nDownloading large-folder/13935_V3.ass to /work/ass_files/13935_V3.ass...\nDownloaded large-folder/13935_V3.ass\nDownloading large-folder/13967_V3.ass to /work/ass_files/13967_V3.ass...\nDownloaded large-folder/13967_V3.ass\nDownloading large-folder/13969_V3.ass to /work/ass_files/13969_V3.ass...\nDownloaded large-folder/13969_V3.ass\nDownloading large-folder/13972_V3.ass to /work/ass_files/13972_V3.ass...\nDownloaded large-folder/13972_V3.ass\nDownloading large-folder/14022_V3.ass to /work/ass_files/14022_V3.ass...\nDownloaded large-folder/14022_V3.ass\nDownloading large-folder/14081_V3.ass to /work/ass_files/14081_V3.ass...\nDownloaded large-folder/14081_V3.ass\nDownloading large-folder/14120_V3.ass to /work/ass_files/14120_V3.ass...\nDownloaded large-folder/14120_V3.ass\nDownloading large-folder/14122_V3.ass to /work/ass_files/14122_V3.ass...\nDownloaded large-folder/14122_V3.ass\nDownloading large-folder/14123_V3.ass to /work/ass_files/14123_V3.ass...\nDownloaded large-folder/14123_V3.ass\nReached the limit of 100 files. Stopping.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "6defc52",
        "execution_start": 1733266438633,
        "execution_millis": 1,
        "execution_context_id": "9ff8ea37-5664-4ca5-a559-0b2cdacda485",
        "cell_id": "7dc2705665b94de6a010e7943c96f8c2",
        "deepnote_cell_type": "code",
        "id": "vbw5Rxhuy_JL"
      },
      "source": [
        "# parse_ass_file(input_file)\n",
        "\n",
        "def parse_ass_file(input_file, output_directory):\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Extract the file ID from the header\n",
        "    file_id = None\n",
        "    for line in lines:\n",
        "        match = re.search(r\"File:\\s.*?(\\d+)\\.mp4\", line)\n",
        "        if match:\n",
        "            file_id = match.group(1)\n",
        "            break\n",
        "\n",
        "    if not file_id:\n",
        "        raise ValueError(f\"File ID not found in the header of file {input_file}.\")\n",
        "\n",
        "    # Prepare the output filename\n",
        "    output_file = os.path.join(output_directory, f\"{file_id}.txt\")\n",
        "\n",
        "    # Extract the start and text columns from the [Events] section\n",
        "    events_started = False\n",
        "    rows = []\n",
        "    for line in lines:\n",
        "        if \"[Events]\" in line:\n",
        "            events_started = True\n",
        "            continue\n",
        "\n",
        "        if events_started:\n",
        "            # Match Dialogue lines\n",
        "            match = re.match(r\"Dialogue:\\s\\d+,(.*?),(.*?),Default,.*?,(.*)\", line)\n",
        "            if match:\n",
        "                start_time = match.group(1).strip()\n",
        "                text = match.group(3).strip()\n",
        "                rows.append(f\"{start_time},{text}\")\n",
        "\n",
        "    # Write to output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(\"\\n\".join(rows))\n",
        "\n",
        "    print(f\"File '{output_file}' created successfully with {len(rows)} rows.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "969b33f4",
        "execution_start": 1733271590291,
        "execution_millis": 46910,
        "execution_context_id": "1dc59ed5-95cc-4e36-a27a-f876269102bd",
        "cell_id": "944d8f9de56346c1ba8737961f4909f4",
        "deepnote_cell_type": "code",
        "id": "QpZMEpgCy_JM",
        "outputId": "dc854c86-a8e2-44a5-a43f-5cb5d6efce57"
      },
      "source": [
        "def time_to_seconds(timestamp):\n",
        "    \"\"\"\n",
        "    Convert a timestamp (hh:mm:ss.xx) to seconds.\n",
        "    \"\"\"\n",
        "    parts = timestamp.split(\":\")\n",
        "    hours = int(parts[0])\n",
        "    minutes = int(parts[1])\n",
        "    seconds = float(parts[2])\n",
        "    return hours * 3600 + minutes * 60 + seconds\n",
        "\n",
        "def parse_ass_file(input_file, output_directory):\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Extract the file ID from the header\n",
        "    file_id = None\n",
        "    for line in lines:\n",
        "        match = re.search(r\"File:\\s.*?(\\d+)\\.mp4\", line)\n",
        "        if match:\n",
        "            file_id = match.group(1)\n",
        "            break\n",
        "\n",
        "    if not file_id:\n",
        "        raise ValueError(f\"File ID not found in the header of file {input_file}.\")\n",
        "\n",
        "    # Prepare the output filename\n",
        "    output_file = os.path.join(output_directory, f\"{file_id}.txt\")\n",
        "\n",
        "    # Extract the start time and text columns from the [Events] section\n",
        "    events_started = False\n",
        "    rows = []\n",
        "    for line in lines:\n",
        "        if \"[Events]\" in line:\n",
        "            events_started = True\n",
        "            continue\n",
        "\n",
        "        if events_started:\n",
        "            # Match Dialogue lines and extract the first timestamp and the text\n",
        "            match = re.match(r\"Dialogue:\\s\\d+,(.*?),.*?,Default,.*?,.*?,.*?,(.*)\", line)\n",
        "            if match:\n",
        "                start_time = match.group(1).strip()  # Only the first timestamp\n",
        "                start_time_seconds = time_to_seconds(start_time)  # Convert to seconds\n",
        "                text = match.group(2).strip()       # The text content\n",
        "                rows.append(f\"{start_time_seconds:.2f},{text}\")\n",
        "\n",
        "    # Write to output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(\"\\n\".join(rows))\n",
        "\n",
        "    print(f\"File '{output_file}' created successfully with {len(rows)} rows.\")\n",
        "\n",
        "def process_ass_files(input_directory, output_directory):\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    # Iterate over all files in the input directory\n",
        "    for filename in os.listdir(input_directory):\n",
        "        if filename.endswith('.ass'):\n",
        "            input_file = os.path.join(input_directory, filename)\n",
        "            try:\n",
        "                parse_ass_file(input_file, output_directory)\n",
        "            except ValueError as e:\n",
        "                print(e)  # Log the error if file ID not found\n",
        "\n",
        "\n",
        "input_directory = '/work/ass_files/'\n",
        "output_directory = '/work/ground_truth/'\n",
        "\n",
        "# Process all .ass files\n",
        "process_ass_files(input_directory, output_directory)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "File '/work/ground_truth/10284.txt' created successfully with 33 rows.\nFile '/work/ground_truth/10417.txt' created successfully with 50 rows.\nFile '/work/ground_truth/1047.txt' created successfully with 89 rows.\nFile '/work/ground_truth/10617.txt' created successfully with 44 rows.\nFile '/work/ground_truth/10662.txt' created successfully with 124 rows.\nFile '/work/ground_truth/10686.txt' created successfully with 80 rows.\nFile '/work/ground_truth/10739.txt' created successfully with 23 rows.\nFile '/work/ground_truth/10788.txt' created successfully with 22 rows.\nFile '/work/ground_truth/10932.txt' created successfully with 47 rows.\nFile '/work/ground_truth/10962.txt' created successfully with 49 rows.\nFile '/work/ground_truth/10965.txt' created successfully with 49 rows.\nFile '/work/ground_truth/10966.txt' created successfully with 28 rows.\nFile '/work/ground_truth/11018.txt' created successfully with 29 rows.\nFile '/work/ground_truth/11027.txt' created successfully with 57 rows.\nFile '/work/ground_truth/11064.txt' created successfully with 45 rows.\nFile '/work/ground_truth/11067.txt' created successfully with 19 rows.\nFile '/work/ground_truth/11094.txt' created successfully with 27 rows.\nFile '/work/ground_truth/11099.txt' created successfully with 83 rows.\nFile '/work/ground_truth/11108.txt' created successfully with 26 rows.\nFile '/work/ground_truth/11141.txt' created successfully with 53 rows.\nFile '/work/ground_truth/11173.txt' created successfully with 34 rows.\nFile '/work/ground_truth/11196.txt' created successfully with 56 rows.\nFile '/work/ground_truth/11198.txt' created successfully with 80 rows.\nFile '/work/ground_truth/11205.txt' created successfully with 50 rows.\nFile '/work/ground_truth/11405.txt' created successfully with 48 rows.\nFile '/work/ground_truth/11410.txt' created successfully with 28 rows.\nFile '/work/ground_truth/11432.txt' created successfully with 42 rows.\nFile '/work/ground_truth/11580.txt' created successfully with 62 rows.\nFile '/work/ground_truth/11595.txt' created successfully with 59 rows.\nFile '/work/ground_truth/11626.txt' created successfully with 18 rows.\nFile '/work/ground_truth/11641.txt' created successfully with 46 rows.\nFile '/work/ground_truth/11658.txt' created successfully with 59 rows.\nFile '/work/ground_truth/11815.txt' created successfully with 33 rows.\nFile '/work/ground_truth/11818.txt' created successfully with 45 rows.\nFile '/work/ground_truth/11890.txt' created successfully with 19 rows.\nFile '/work/ground_truth/11892.txt' created successfully with 18 rows.\nFile ID not found in the header of file /work/ass_files/11904_V3.ass.\nFile '/work/ground_truth/12013.txt' created successfully with 61 rows.\nFile '/work/ground_truth/12028.txt' created successfully with 61 rows.\nFile '/work/ground_truth/12030.txt' created successfully with 14 rows.\nFile '/work/ground_truth/12037.txt' created successfully with 39 rows.\nFile ID not found in the header of file /work/ass_files/12121_V3.ass.\nFile '/work/ground_truth/12164.txt' created successfully with 51 rows.\nFile '/work/ground_truth/12205.txt' created successfully with 39 rows.\nFile '/work/ground_truth/12254.txt' created successfully with 85 rows.\nFile '/work/ground_truth/12295.txt' created successfully with 30 rows.\nFile '/work/ground_truth/12305.txt' created successfully with 46 rows.\nFile '/work/ground_truth/12367.txt' created successfully with 59 rows.\nFile '/work/ground_truth/12412.txt' created successfully with 32 rows.\nFile '/work/ground_truth/12429.txt' created successfully with 42 rows.\nFile '/work/ground_truth/12478.txt' created successfully with 60 rows.\nFile '/work/ground_truth/12480.txt' created successfully with 47 rows.\nFile '/work/ground_truth/12562.txt' created successfully with 44 rows.\nFile '/work/ground_truth/12570.txt' created successfully with 29 rows.\nFile '/work/ground_truth/12572.txt' created successfully with 45 rows.\nFile '/work/ground_truth/12612.txt' created successfully with 165 rows.\nFile '/work/ground_truth/12616.txt' created successfully with 25 rows.\nFile '/work/ground_truth/12619.txt' created successfully with 79 rows.\nFile '/work/ground_truth/12633.txt' created successfully with 32 rows.\nFile '/work/ground_truth/12643.txt' created successfully with 65 rows.\nFile '/work/ground_truth/1264.txt' created successfully with 63 rows.\nFile '/work/ground_truth/12679.txt' created successfully with 29 rows.\nFile '/work/ground_truth/12686.txt' created successfully with 49 rows.\nFile '/work/ground_truth/12706.txt' created successfully with 54 rows.\nFile '/work/ground_truth/12710.txt' created successfully with 69 rows.\nFile '/work/ground_truth/12711.txt' created successfully with 28 rows.\nFile '/work/ground_truth/12756.txt' created successfully with 57 rows.\nFile '/work/ground_truth/12757.txt' created successfully with 68 rows.\nFile '/work/ground_truth/12776.txt' created successfully with 59 rows.\nFile '/work/ground_truth/12789.txt' created successfully with 67 rows.\nFile '/work/ground_truth/12873.txt' created successfully with 59 rows.\nFile '/work/ground_truth/12877.txt' created successfully with 58 rows.\nFile '/work/ground_truth/12931.txt' created successfully with 59 rows.\nFile '/work/ground_truth/12964.txt' created successfully with 20 rows.\nFile ID not found in the header of file /work/ass_files/13016_V3.ass.\nFile ID not found in the header of file /work/ass_files/13017_V3.ass.\nFile '/work/ground_truth/13018.txt' created successfully with 27 rows.\nFile ID not found in the header of file /work/ass_files/13069_V2.ass.\nFile '/work/ground_truth/13103.txt' created successfully with 47 rows.\nFile '/work/ground_truth/13117.txt' created successfully with 40 rows.\nFile '/work/ground_truth/13226.txt' created successfully with 30 rows.\nFile '/work/ground_truth/13283.txt' created successfully with 86 rows.\nFile '/work/ground_truth/13397.txt' created successfully with 33 rows.\nFile '/work/ground_truth/13407.txt' created successfully with 87 rows.\nFile '/work/ground_truth/13497.txt' created successfully with 51 rows.\nFile '/work/ground_truth/13647.txt' created successfully with 25 rows.\nFile '/work/ground_truth/13741.txt' created successfully with 23 rows.\nFile '/work/ground_truth/13742.txt' created successfully with 47 rows.\nFile '/work/ground_truth/13859.txt' created successfully with 38 rows.\nFile '/work/ground_truth/13888.txt' created successfully with 28 rows.\nFile '/work/ground_truth/13914.txt' created successfully with 123 rows.\nFile '/work/ground_truth/13935.txt' created successfully with 60 rows.\nFile '/work/ground_truth/13967.txt' created successfully with 50 rows.\nFile '/work/ground_truth/13969.txt' created successfully with 73 rows.\nFile '/work/ground_truth/13972.txt' created successfully with 53 rows.\nFile '/work/ground_truth/14022.txt' created successfully with 38 rows.\nFile '/work/ground_truth/14081.txt' created successfully with 35 rows.\nFile ID not found in the header of file /work/ass_files/14120_V3.ass.\nFile '/work/ground_truth/14122.txt' created successfully with 57 rows.\nFile '/work/ground_truth/14123.txt' created successfully with 50 rows.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining Feature Extraction Functions"
      ],
      "metadata": {
        "id": "BiXWiSYUIgge"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "3b1a23a8",
        "execution_start": 1732682337668,
        "execution_millis": 1,
        "execution_context_id": "b46e7722-b202-4b63-9db6-eb0968a463f9",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "16c94f6516fd4dec857b1d09de5129cb",
        "deepnote_cell_type": "code",
        "id": "gQf-1g4Fy_JN"
      },
      "source": [
        "# F_0 estimation\n",
        "\n",
        "def estimate_pitch(y, sr, voicing_threshold=0.3, use_viterbi=False):\n",
        "    \"\"\"\n",
        "    Estimate the fundamental frequency (pitch) of an audio file using the CREPE algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y: audio samples\n",
        "    sr: sampling rate\n",
        "    voicing_threshold : float, optional\n",
        "        The confidence threshold above which a frame is considered voiced. Frames with confidence\n",
        "        levels below this threshold are marked as unvoiced (i.e., set to 0 Hz).\n",
        "        Default is 0.3.\n",
        "    use_viterbi : bool, optional\n",
        "        If True, apply Viterbi decoding to smooth the pitch track and obtain more consistent\n",
        "        pitch estimates over time. Default is False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    time : np.ndarray\n",
        "        A 1D numpy array containing time stamps for each frame in seconds.\n",
        "    frequency : np.ndarray\n",
        "        A 1D numpy array containing the estimated pitch for each frame in Hz. Unvoiced frames\n",
        "        are set to 0 Hz.\n",
        "    confidence : np.ndarray\n",
        "        A 1D numpy array containing the confidence of the pitch estimate for each frame.\n",
        "    activation : np.ndarray\n",
        "        A 2D numpy array representing the activation matrix returned by the CREPE algorithm,\n",
        "        which can be used to visualize the pitch estimation process.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    time, frequency, confidence, activation = crepe.predict(y, sr, viterbi=use_viterbi)\n",
        "    frequency[confidence < voicing_threshold] = 0 # so confidence is the max of each row of activation, where each row is a time stamp\n",
        "\n",
        "    return time, frequency, confidence, activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8834c302",
        "execution_start": 1732682400205,
        "execution_millis": 1043749,
        "execution_context_id": "b46e7722-b202-4b63-9db6-eb0968a463f9",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "c6a9ba37b6494375b29240d75f6da683",
        "deepnote_cell_type": "code",
        "id": "jLu9LtNVy_JN",
        "outputId": "34437172-bf9b-49cd-d67b-82c289cd02bd"
      },
      "source": [
        "def generate_audio_features(audio_filename, output_dir):\n",
        "\n",
        "    y, sr = librosa.load(audio_filename, sr=None)\n",
        "    duration = librosa.get_duration(y=y, sr=sr)\n",
        "    segment_length = 30  # Segment length in seconds\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Dictionary to store results\n",
        "    audio_features = {}\n",
        "\n",
        "    # Divide audio into 30-second segments\n",
        "    num_segments = int(np.ceil(duration / segment_length))\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        start_sample = int(i * segment_length * sr)\n",
        "        end_sample = int(min((i + 1) * segment_length * sr, len(y)))\n",
        "        segment = y[start_sample:end_sample]\n",
        "\n",
        "        # Generate spectrogram\n",
        "        S = librosa.stft(segment, hop_length = sr*0.01)\n",
        "        S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
        "\n",
        "        # Extract MFCCs\n",
        "        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=20, hop_length = sr*0.01)\n",
        "\n",
        "        # F_0 estimation\n",
        "        time, frequency, confidence, activation = estimate_pitch(segment, sr, voicing_threshold=0.3, use_viterbi=True)\n",
        "\n",
        "        file_name, file_extension = os.path.splitext(audio_filename)\n",
        "\n",
        "        # Create the segment name\n",
        "        segment_name = f\"{file_name}_segment_{i+1}{file_extension}\"\n",
        "\n",
        "        audio_features[segment_name] = {\n",
        "            \"spectrogram\": S_db,\n",
        "            \"mfcc\": mfccs[1:], # remove first row of mfcc\n",
        "            \"f_0 estimation\": frequency\n",
        "        }\n",
        "\n",
        "    output_file = os.path.join(output_dir, f\"{file_name}_features.pkl\")\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(audio_features, f)\n",
        "\n",
        "\n",
        "    return audio_features\n",
        "\n",
        "features = generate_audio_features('/work/11108_v360P_V1_source_3.wav', 'output_features')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-11-27 04:40:00.769927: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-11-27 04:40:01.019685: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2024-11-27 04:40:01.019712: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2024-11-27 04:40:01.045602: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-11-27 04:40:01.772015: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2024-11-27 04:40:01.772087: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2024-11-27 04:40:01.772094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2024-11-27 04:40:02.982825: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2024-11-27 04:40:02.982851: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2024-11-27 04:40:02.982870: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-6e56d0c4-9a28-43e5-8909-f89e80202117): /proc/driver/nvidia/version does not exist\n2024-11-27 04:40:02.986266: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n94/94 [==============================] - 124s 1s/step\n94/94 [==============================] - 123s 1s/step\n94/94 [==============================] - 122s 1s/step\n94/94 [==============================] - 122s 1s/step\n94/94 [==============================] - 120s 1s/step\n94/94 [==============================] - 122s 1s/step\n94/94 [==============================] - 121s 1s/step\n72/72 [==============================] - 92s 1s/step\n",
          "output_type": "stream"
        }
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "1366a20293eb4a67a6925a5e0f5d0515",
    "colab": {
      "provenance": []
    }
  }
}